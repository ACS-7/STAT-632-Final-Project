---
title: "STAT 632 Final Project"
author: "Andrew Stanciulescu"
date: "2023-04-23"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# load necessary packages
```{r}
library(tidyverse)
library(MASS)
library(car)
library(pROC)
library(e1071)
library(caTools)
library(class)
library(tidymodels)
```

# read in dataset, remove User.ID, split into training and test data
```{r}
car = read.csv("car_data.csv")

set.seed(777)

car$id = 1:nrow(car)

train = car %>% sample_frac(0.70)
test = anti_join(car, train, by = 'id')

train = subset(train, select = -c(User.ID, id))
test = subset(test, select = -c(User.ID, id))

head(train)
head(test)
```

# descriptive statistics
```{r}
summary(train)
summary(test)
```

# visualizations of Age
```{r}
ggplot(train, aes(x = Age)) +
  geom_histogram(aes(y = ..density..), bins = 30, color = "black", fill = "#06388a") +
  theme_light() +
  geom_density(alpha = 0.3, size = 1.5, col = "#751919") +
  ggtitle("Training Data") +
  theme(plot.title = element_text(hjust = 0.5, size = 22, face = "bold"), 
        axis.title = element_text(size = 17, face = "bold"), 
        axis.text = element_text(size=12, face = "bold")) +
  xlab("Age") +
  ylab("Density")

ggplot(test, aes(x = Age)) +
  geom_histogram(aes(y = ..density..), bins = 30, color = "black", fill = "#06388a") +
  theme_light() +
  geom_density(alpha = 0.3, size = 1.5, col = "#751919") +
  ggtitle("Test Data") +
  theme(plot.title = element_text(hjust = 0.5, size = 22, face = "bold"), 
        axis.title = element_text(size = 17, face = "bold"), 
        axis.text = element_text(size=12, face = "bold")) +
  xlab("Age") +
  ylab("Density")
```

# visualization of income
```{r}
ggplot(train, aes(x = AnnualSalary)) +
  geom_histogram(aes(y = ..density..), bins = 30, color = "black", fill = "#06388a") +
  theme_light() +
  geom_density(alpha = 0.3, size = 1.5, col = "#751919") +
  ggtitle("Training Data") +
  theme(plot.title = element_text(hjust = 0.5, size = 22, face = "bold"), 
        axis.title = element_text(size = 17, face = "bold"), 
        axis.text.y = element_blank(),
        axis.text.x = element_text(size=12, face = "bold"))+
  scale_x_continuous(limits = c(10000,150000)) +
  xlab("Annual Income") +
  ylab("Density")

ggplot(test, aes(x = AnnualSalary)) +
  geom_histogram(aes(y = ..density..), bins = 30, color = "black", fill = "#06388a") +
  theme_light() +
  geom_density(alpha = 0.3, size = 1.5, col = "#751919") +
  ggtitle("Test Data") +
  theme(plot.title = element_text(hjust = 0.5, size = 22, face = "bold"), 
        axis.title = element_text(size = 17, face = "bold"), 
        axis.text.y = element_blank(),
        axis.text.x = element_text(size=12, face = "bold"))+
  scale_x_continuous(limits = c(10000,150000)) +
  xlab("Annual Income") +
  ylab("Density")
```

# scatterplot matrix for the dataset, convert Gender to numeric just for this part
```{r}
train_numeric = train
train_numeric$Gender = ifelse(train_numeric$Gender == "Female",0,1)

test_numeric = test
test_numeric$Gender = ifelse(test_numeric$Gender == "Female",0,1)

head(train_numeric)

pairs(Purchased ~ ., data = train_numeric)
```

# perform logistic regression - FULL MODEL
```{r}
glm1 = glm(Purchased ~ ., data = train, family = 'binomial')
summary(glm1)
```

# Gender is not significant so we will remove it from the model, FINAL MODEL
```{r}
glm2 = step(glm1, trace = 0)
summary(glm2)
```

# An example with a 22 year old Female with a salary of $125000
```{r}
newdata = data.frame(Gender = "Female", Age = 22, AnnualSalary = 125000)
predict(glm2, newdata, type = 'response')
```

# Area under the curve
```{r}
results = predict(glm2, test, type = 'response') 
roc_obj = roc(test$Purchased, results)
plot(roc_obj, print.auc = TRUE)
```

# Finding out how accurate our logistic regression is
```{r}
glm3 = glm(Purchased ~ Age + AnnualSalary, data = train, family = binomial)
results3 = predict(glm3, test, type = 'response') 
results3 =ifelse(results3 > 0.5,1,0)
correct = test$Purchased
accuracy =  mean(correct == results3)
accuracy
```

----------------------------------

# KNN

```{r}
train_scale = scale(train_numeric[, 1:3])
test_scale = scale(test_numeric[, 1:3])

k = list()
n = seq(1,100)
for (i in 1:100){
  knn = knn(train = train_scale,
          test = test_scale,
          cl = train$Purchased,
          k = i)
  misClassError = mean(knn != test$Purchased)
  print(paste("K-value: ", i, "|", 'Accuracy =', 1-misClassError))
  k[i] = 1- misClassError
}

nk = data.frame(unlist(n), unlist(k))


nk$unlist.k. = round(nk$unlist.k., 3)

nk
ggplot(nk, aes(unlist.n., unlist.k.)) +
  geom_point(color = "blue") +
  theme_bw() + 
  ggtitle("Accuracy vs. K-Value") +
  theme(plot.title = element_text(hjust = 0.5)) +
  xlab("K-Value") +
  ylab("Accuracy")
  

plot(n,k)

mean(nk$unlist.k.)
```

----------------------------------

# Boosted C5.0

```{r}
k = list()
n = seq(1,100)
for (i in 1:100){
  C50 <- boost_tree(trees = i) %>% 
  set_engine("C5.0") %>%
  set_mode("classification") %>%
  fit(as.factor(Purchased) ~ ., data = train)
  pred = predict(C50, test)
  misClassErrorC50 = mean(pred != test$Purchased)
  k[i] = 1 - misClassErrorC50
}

nk = data.frame(unlist(n), unlist(k))


nk$unlist.k. = round(nk$unlist.k., 3)

nk
ggplot(nk, aes(unlist.n., unlist.k.)) +
  geom_point(color = "blue") +
  theme_bw() + 
  ggtitle("Accuracy vs. n-Trees") +
  theme(plot.title = element_text(hjust = 0.5)) +
  xlab("n-Trees") +
  ylab("Accuracy")
```














































